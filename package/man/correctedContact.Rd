\name{correctedContact}
\alias{correctedContact}

\title{Iterative correction of Hi-C counts}

\description{Perform iterative correction on counts for Hi-C interactions to correct for biases between fragments.}

\usage{
correctedContact(data, iterations=50, exclude.local=1, ignore.low=0.02, 
    winsor.high=0.02, average=TRUE, dispersion=0.05)
}

\arguments{
	\item{data}{a \code{DIList} object produced by \code{\link{squareCounts}}}
	\item{iterations}{an integer scalar specifying the number of correction iterations}
	\item{exclude.local}{an integer scalar, indicating the distance off the diagonal under which bin pairs are excluded}
	\item{ignore.low}{a numeric scalar, indicating the proportion of low-abundance bins to ignore}
	\item{winsor.high}{a numeric scalar indicating the proportion of high-abundance bin pairs to winsorize}
	\item{average}{a logical scalar specifying whether the average count should be used for correction}
	\item{dispersion}{a numeric scalar for use in computing the average count in \code{\link{mglmOneGroup}}}
}

\value{
A list with several components. 
Each component is a vector if \code{average=TRUE} and a matrix otherwise, with the number of columns in the latter equal to the number of libraries in \code{data}:
	\item{truth}{a numeric vector or matrix containing the true interaction probabilities for each bin pair}
	\item{bias}{a numeric vector or matrix of biases for all bins}
    \item{max}{a numeric vector or matrix containing the maximum fold-change change in biases at each iteration}
}

\details{
This function implements the iterative correction procedure described by
Imakaev \emph{et al.} in their 2012 paper. Briefly, this aims to factorize
the count for each bin pair into the bias for the anchor bin, the bias
for the target bin and the true interaction probability. The probability
sums to 1 across all bin pairs for a given bin. The bias represents
the ease of sequencing/mapping/other for that genomic region.

The \code{data} argument should be generated by taking the output of \code{\link{squareCounts}} after setting \code{filter=1}. 
Filtering should be avoided as counts in low-abundance bin pairs may be informative upon summation for each bin.
For example, a large count sum for a bin may be formed from many bin pairs with low counts.
Removal of those bin pairs would result in the loss of per-bin information.

Some robustness is provided by winsorizing out strong interactions with
\code{winsor.high} to ensure that they do not overly influence the computed
biases/probabilities. Low-abundance bins can also be removed with
\code{ignore.low} to avoid instability during correction, though this will
result in \code{NA} values in the output.

Local bin pairs can be excluded as these are typically irrelevant to long-range
interactions. They are also typically very high-abundance and may have
excessive weight during correction, if not removed.  This can be done by
removing all bin pairs where the difference between the anchor and target
indices is less than \code{exclude.local}.

If multiple libraries are used to generate \code{data}, and \code{average=TRUE}, an average count will be computed for each bin pairs across all libraries using \code{\link{mglmOneGroup}} with the specified \code{dispersion}.
The average count will then be used for correction.
Otherwise, correction will be performed separately for each library in \code{data} and the output will be returned in the form of matrices.
Each column of each matrix will correspond to a library in \code{data}.

The maximum step size in the output can be used as a measure of convergence.
Ideally, the step size should approach 1 as iterations pass. This indicates
that the correction procedure is converging to a single solution.

If not enough memory is available to hold all non-empty bin pairs, users should use a larger \code{width} in \code{\link{squareCounts}}.
The original smaller bins can be overlapped with the new larger bins.
The bias of the former can then be used as the bias of the latter.
Dividing the count by the product of the biases will obtain the estimated contact probability.

% Note that the true signals are not provided directly. This is because one usualy applies the correction to counts in a separate object. For 
% example, larger counts and \code{width} values are required during correction but smaller squares may be of interest during the differential
% analysis. Biases from the former can be applied to the latter with \code{\link{getBias}}. 

% True signals are continuous variables and have limited use in count-based statistical frameworks.
% You need to compute the bias for each one to get the offset.
}

\examples{
hic.file <- system.file("exdata", "hic_sort.bam", package="diffHic")
cuts <- readRDS(system.file("exdata", "cuts.rds", package="diffHic"))

# Setting up the parameters
fout <- "output"
preparePairs(hic.file, cuts, fout)
countPairs(fout)
data <- squareCounts(fout, cuts, width=50, filter=1L)

# Correcting.
stuff <- correctedContact(data)
head(stuff$truth)
head(stuff$bias)
plot(stuff$max)

unlink(fout, recursive=TRUE)
}

\author{Aaron Lun}

\seealso{
\code{\link{squareCounts}}, 
%\code{\link{getBias}},
\code{\link{mglmOneGroup}}
}

\references{
Imakaev M et al. (2012). Iterative correction of Hi-C data reveals hallmarks of chromosome organization. \emph{Nat. Methods} 9, 999-1003.
}

